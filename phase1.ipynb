{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w67mZW1i333",
    "tags": []
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ohy-uGrhk_6j"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES23zLXhi34A",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C72evWemRM_"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "  image_pickle_file_path = 'images.pkl'\n",
    "  label_pickle_file_path = 'label.pkl'\n",
    "\n",
    "  with open(image_pickle_file_path, 'rb') as file:\n",
    "    images = pickle.load(file)\n",
    "\n",
    "  with open(label_pickle_file_path, 'rb') as file:\n",
    "    labels = pickle.load(file)\n",
    "\n",
    "  # images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PegQSxmRnEAU"
   },
   "outputs": [],
   "source": [
    "images, labels = load_dataset()\n",
    "random_indices = np.random.choice(560, size=20, replace=False)\n",
    "# Extract the randomly selected values\n",
    "random_values = images[random_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4EkzGxIi34C"
   },
   "source": [
    "## Proccess on images\n",
    "- Extract features\n",
    "- normalize features\n",
    "- make features a single dimnetion vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niz6RZqTi34D"
   },
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    original_features = []\n",
    "\n",
    "    # For each image in the dataset\n",
    "    for img in images:\n",
    "\n",
    "        height, width, _ = img.shape\n",
    "        img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # For each pixel in the image\n",
    "        for i in range(img_hsv.shape[0]):\n",
    "            for j in range(img_hsv.shape[1]):\n",
    "                pixel = img_hsv[i, j]\n",
    "                h, s, v = pixel\n",
    "                x, y = i, j\n",
    "\n",
    "                original_features.append((h,s,v,x,y))\n",
    "\n",
    "                h_weight = 10\n",
    "                s_weight = 1\n",
    "                v_weight = 1\n",
    "                x_weight = 0\n",
    "                y_weight = 0\n",
    "                h, s, v = h * h_weight, s * s_weight, v * v_weight\n",
    "                x, y = x * x_weight, y * y_weight\n",
    "\n",
    "                features.append((h, s, v, x, y))\n",
    "\n",
    "    return features, original_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thgfwhB_i34F"
   },
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    features_array = np.array(features)\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features_array = scaler.fit_transform(features_array)\n",
    "    normalized_features = normalized_features_array.tolist()\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPSriJDwi34G"
   },
   "outputs": [],
   "source": [
    "def process_images_in_chunks(images, chunk_size):\n",
    "    features = []\n",
    "    original_features = []\n",
    "    num_images = len(images)\n",
    "\n",
    "    for i in range(0, num_images, chunk_size):\n",
    "        # Extract features for a chunk of images\n",
    "        chunk_features, chunk_original_features = extract_features(images[i:i+chunk_size])\n",
    "\n",
    "        # Append the features to the overall features list\n",
    "        features.extend(chunk_features)\n",
    "        original_features.extend(chunk_original_features)\n",
    "\n",
    "        print('<<', i, '>>')\n",
    "\n",
    "    return features, original_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zo_qGUNi34H",
    "outputId": "44412444-40e7-41ab-e547-d11217b7b25f"
   },
   "outputs": [],
   "source": [
    "# extract features\n",
    "chunk_size = 10\n",
    "# features, original_features = process_images_in_chunks(images, chunk_size)\n",
    "features, original_features = process_images_in_chunks(random_values, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_vQuK0ii34J"
   },
   "outputs": [],
   "source": [
    "# normalize features\n",
    "features = normalize_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4kDj4dsi34K"
   },
   "source": [
    "## clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yxNa4MT9EQ_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3BrJgCQi34K"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "clusters = kmeans.fit_predict(features)\n",
    "# clusters = clusters.reshape(images.shape[0], images.shape[1], images.shape[2])\n",
    "clusters = clusters.reshape(random_values.shape[0], random_values.shape[1], random_values.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UJ8UdX6i34P"
   },
   "source": [
    "## display clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFG_Oueri34L"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean color for each cluster\n",
    "def cluster_mean_value(kmeans, clusters):\n",
    "    cluster_means = []\n",
    "    for cluster in range(kmeans.n_clusters):\n",
    "        mask = (clusters == cluster)\n",
    "        cluster_mean = np.mean(random_values * mask[:, :, :, np.newaxis], axis=(0, 1, 2)) / np.mean(mask)\n",
    "        cluster_means.append(cluster_mean)\n",
    "    # Convert the mean colors to integer values\n",
    "    cluster_means = np.round(cluster_means).astype(int)\n",
    "    return cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nH5Vrm4_i34L"
   },
   "outputs": [],
   "source": [
    "clusters_mean = cluster_mean_value(kmeans, clusters)\n",
    "# clusters_mean.reshape(clusters_mean.shape[0]*clusters_mean.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2YYqr8Ri34P"
   },
   "outputs": [],
   "source": [
    "def display_clusters(cluster_means):\n",
    "    # Display images with cluster colors\n",
    "    for i, image in enumerate(random_values):\n",
    "        cv2.imshow('Original', image)\n",
    "\n",
    "        # Create a blank image for displaying clustered colors\n",
    "        cluster_image = np.zeros_like(image)\n",
    "        \n",
    "\n",
    "        # Assign the mean color to each pixel based on the cluster assignment\n",
    "        for cluster, mean_color in enumerate(cluster_means):\n",
    "            mask = (clusters[i] == cluster)\n",
    "            cluster_image[mask] = mean_color\n",
    "\n",
    "        cv2.imshow('Clustered', cluster_image)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "display_clusters(clusters_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi_CypDqi34L"
   },
   "source": [
    "## extracting clusters features for each image sepratedly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean color for each cluster in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_mean(first_array, second_array):\n",
    "    \n",
    "    mean_values = []\n",
    "    unique_elements = np.unique(first_array)\n",
    "        \n",
    "    for element in unique_elements:\n",
    "        indices = np.where(first_array == element)\n",
    "        mean_hsv = np.mean(second_array[indices], axis=0)\n",
    "        mean_values.append(mean_hsv.tolist())\n",
    "\n",
    "    return mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_color_each_cluster_each_image = []\n",
    "for i in range(random_values.shape[0]):\n",
    "    mean_values = calculate_mean(clusters[i], images[i])\n",
    "    mean_color_each_cluster_each_image.append(np.array(mean_values))\n",
    "\n",
    "mean_color_each_cluster_each_image = np.array(mean_color_each_cluster_each_image)\n",
    "print(len(mean_color_each_cluster_each_image))\n",
    "mean_color_each_cluster_each_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance color for each cluster in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_variance(first_array, second_array):\n",
    "    \n",
    "    variance_values = []\n",
    "    unique_elements = np.unique(first_array)\n",
    "\n",
    "    for element in unique_elements:\n",
    "        indices = np.where(first_array == element)\n",
    "        var_hsv = np.var(second_array[indices], axis=0)\n",
    "        variance_values.append(var_hsv.tolist())\n",
    "\n",
    "    return variance_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_color_each_cluster_each_image = []\n",
    "for i in range(random_values.shape[0]):\n",
    "    var_values = calculate_variance(clusters[i], random_values[i])\n",
    "    var_color_each_cluster_each_image.append(np.array(var_values))\n",
    "\n",
    "var_color_each_cluster_each_image = np.array(var_color_each_cluster_each_image)\n",
    "var_color_each_cluster_each_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### size of clusters in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_size(clusters):\n",
    "    size_each_cluster_each_image = []\n",
    "\n",
    "    for i in range(clusters.shape[0]):\n",
    "        unique_clusters, counts = np.unique(clusters[i], return_counts=True)\n",
    "        each_clusters_size = counts\n",
    "        size_each_cluster_each_image.append(each_clusters_size)\n",
    "\n",
    "    size_each_cluster_each_image = np.array(size_each_cluster_each_image)\n",
    "    return size_each_cluster_each_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_each_cluster_each_image = calculate_size(clusters)\n",
    "size_each_cluster_each_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFn0nBVDi34f"
   },
   "source": [
    "## create clusters feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11jURYBei34f"
   },
   "outputs": [],
   "source": [
    "def create_clusters_vector(clusters_mean, clusters_variance, clusters_size, clusters_area=None):\n",
    "    result = []\n",
    "    for mean, variance, size in zip(clusters_mean, clusters_variance, clusters_size):\n",
    "        for i in range(3):\n",
    "            result.append([*mean[i], *variance[i], size])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdoWEeURi34g"
   },
   "outputs": [],
   "source": [
    "def flatten_nested_list(nested_list):\n",
    "    return [[item for sublist in lst for item in (sublist if isinstance(sublist, list) else [sublist])] for lst in nested_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxDRoNKmi34g"
   },
   "outputs": [],
   "source": [
    "clusters_features = create_clusters_vector(mean_color_each_cluster_each_image, var_color_each_cluster_each_image, size_each_cluster_each_image)\n",
    "clusters_features = flatten_nested_list(clusters_features)\n",
    "clusters_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZnhtPpgi34g"
   },
   "source": [
    "## clustering clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJlojCZci34h"
   },
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=3)\n",
    "clusters_features_flat = [np.concatenate([np.array(item[:-1]), item[-1]]) for item in clusters_features]\n",
    "clusters_2 = kmeans.fit_predict(clusters_features_flat)\n",
    "# clusters_2 = kmeans.fit_predict(clusters_features)\n",
    "clusters_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6onYuxUi34h"
   },
   "source": [
    "## Diaplay clustered cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    "data_transformed = pca.fit_transform(clusters_features_flat)\n",
    "\n",
    "plt.scatter(data_transformed[:, 0], data_transformed[:, 1], c=clusters_2)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3mVvdAtI9ONL",
    "lBEBJMkk9Tsz",
    "UcQ9WDY59Zlb",
    "06xiqibEqHid",
    "aWfpqHU1qPh4"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
